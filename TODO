
- currently, the pipeline feeds into less(1) at the end.  better will be to have it feed into /dev/null, and then run less(1) separately, reading from multiple pipe inputs.  these pipe inputs are tee(1)s of the pipes at each stage of the pipeline.  the existing caching mechanism makes that easy, since it's just the same `tail -c +1 --follow --pid "$cache_file".pid "$cache_file".part` trick that's used by _cache_part_2, for active portions of the update pipeline.  for "inactive" portions of the update pipeline (ie. the earlier parts of the pipeline, that have been skipped because the cache is valid), then the pipe can literally just be fed by `cat $cache_file`.  so the less(1) command can (theoretically) just be something like:

    less -f <(cat "$cache_file_1") <(cat "$cache_file_2") <(tail -c +1 --follow --pid "$cache_file_3".pid "$cache_file_3".part)  <(tail -c +1 --follow --pid "$cache_file_4".pid "$cache_file_4".part) ...

  - this does mean that there will now be three bg jobs that the script will be waiting on, instead of the current two (the update pipeline + less, and the inotifywait), being the update pipeline, the less, and the inotifywait.  however this is fine - the behaviour when inotifywait exits first is to kill the update pipeline, and the less (same as now, except there are two jobs to kill instead of one), the behaviour when less exits first is to kill the update pipeline and the inotifywait, and the behaviour when the update pipeline exits first is to just keep waiting for the other two (same as now, basically - when the update pipeline exhausts, less just keeps running as normal).

  - however, running less as above does mean that it will display ugly, useless "filenames" in the statusbar of `/dev/fd/XX`.  to get around that, instead of using `<()`, we can mkfifo(1) actual pipe special files, and then specify those to less(1).  in fact, we can just name the fifos the exact same as the command that's generating the output.  all the characters in it are valid filename characters, except for nul (which we can't do anything about, except to replace with `\0`), and `/`, which we can handle by creating the necessary directory path leading to the fifo itself.  since it's possible for a kenny script to have the exact same command in multiple places (eg. `sort` or `uniq`), this would wreck everything because each fifo needs to be unique.  so we can prepend `N. ` to the front, where N is the stage number of the command in the kenny script (ie. starting from 1).  we also need to clean up these fifos after less(1) exits.  we also don't want to pollute the $PWD with these fifos, so they will need to live in a temp dir belonging to just this instance of kenny (otherwise, separate concurrently running instances of kenny in the same directory, which share some of the same pipeline commands (but not all) will collide and cause mayhem), ie. in a `mktemp -d`.  there's no need to play with $TMPDIR though, they can just go into /tmp or /run or wherever mktemp thinks is best.  or maybe we should force them into /dev/shm if it's a dir, since that's usually tmpfs (whereas /tmp tends to be a real filesystem).  anyway, we can have the less(1) sub-shell just `cd` to this place immediately prior to actually running less(1), so that the relative fifo paths will actually work, as well as looking correct.

  - another problem then is that less(1) will by default show the output from the first file, which is the first stage in the pipeline.  we could get around this by listing the fifos in reverse order, but this would mean that they must be navigated in "mentally reverse" order with the `:n` and `:p` commands.  we could use lesskey(1) to remap `:n` and `:p` to swap their meanings, but we couldn't fix `:x`, because we need `n:x` (for `n` some number) to be interpreted as `N-n:x`, which we can't do by remapping.  so, the solution here is to leave the fifos in normal stage order, and add `+N:x` to the less(1) cmdline, to make less immediately start showing the final fifo, instead of the first.  hopefully this executes immediately, even if the input to the first fifo is slow in coming...  seems like it does, when i tested it.  but even in the absence of this issue, doing `+3:x` does jump to the 3rd file, but also shows a stray `:x` at the start of the output.  the solution to this is to add a literal `^L` to refresh the screen.  so we actually need `+N:x` for `N` the number of output stages.

    older testing: kev@cognique:~$ less -f +b <(date) <(i=0; while [ $i -lt 100 ]; do echo $i; sleep 1; i=$(($i + 1)); done) <(echo bar)

  - actually, just making the fifos is only half the problem.  we also need to feed the data into them.  unfortunately this means yet more bg jobs that need to be waited for, and killed if less/inotifywait exit first.  sigh.  maybe it can be just one sub-shell that does them all (in the bg), and waits for them all, with a trap that kills them all.


- after implementing the above, it would be nice to have less(1) start on the same fifo that the user was looking at previously.  not sure if this is even remotely possible, because how can we get less(1) to tell us which file it had been looking at, prior to being forcibly killed?  we can't make it exit with a special exitcode, because it's being killed, the user's not quitting it.  the custom lesshst file could be an option, except that lesshst by default only stores search history.  i've no idea if there's a way to make it also record the filenames of recently-looked-at files...  it seems like there isn't any way.

  - actually, maybe it can be done by using lesskey(1) to override the `:n`, `:p`, and `:x` commands, so that after they do their navigation, they then run some shell command (via the "extra string", which made to be `!something...`) which somehow records either the navigation, or more ideally, the new current filename that less is viewing.  excellent!  just like vi, this expands `%` to be the current filename.  so it should be possible to make `:n` be `next-file !echo % > some_tmp_file`, and similarly for `:p` and `:x`.  since the lesskey(1) file is going to be generated every time (right?), it should be possible to embed the name of the temp file in there.  however, other ideas are: set the name of the temp file in an exported env var, and then do `!echo % > $SOME_TMP_FILE`.  or, make the temp file be a fifo that a kenny sub-shell is reading from (but, this has the same problem - what's the name of the fifo?).  or, an fd that a kenny sub-shell is reading from (ie. `!echo % 1>&6`, assuming that kenny has created and is listening to (sorry, reading from) fd 6.  this seems like a pretty hard way of doing it.).


- would be nice if, in vim, there was a key binding that jumped to the kenny terminal window, "typed" `X:x` where X is the stage number that the cursor is currently on.  similarly, key bindings for making the kenny-less(1) go to the next/prev fifo, without having to jump to that buffer, do it, and then jump back again.


- cmdline option to open the file in vim, with the kenny terminal window already opened and ready to go.  maybe `kenny edit foo.kenny`?  then `kenny foo.kenny` is a synonym for `kenny run foo.kenny`.  or maybe not, and people just have to do `kenny run foo.kenny` if they want to actually run it.  this sub-command approach is also nice for pretty-printing and stuff, eg. `kenny format foo.kenny` or `kenny script foo.kenny` or `kenny oneliner foo.kenny` ...


- use a dedicated lesshst file, so remembered search patterns are separate from other (regular) uses of less(1)

  - or maybe just no lesshst file at all (`LESSHISTFILE=-` or `LESSHISTFILE=/dev/null`)

- somehow make less(1) save the lesshst when it gets killed.  otherwise, things i've been searching for in the output, get forgotten when less(1) gets reloaded.  is it even possible to acheive this?!  less(1) would have to re-save its lesshst file after every search.  which makes sense in this case, but not in general.  maybe it can be killed with a different signal, that causes a more graceful shutdown?  maybe it does write to the lesshst file, but the new less(1) is starting up too quickly (since the old less(1) is exiting asynchronously)...


- deal properly (somehow) with stderr from the pipeline - currently it just still goes to the terminal, which means when you mess up, it gets output but then quickly hidden when less(1) starts, which is pretty useless.  merging stderr in with stdout (ie. feeding it through the pipeline) is unlikely to be a great solution - if the author intended that, they would have added `2>&1` at the end of the stage.  so, the options seem to be: (a) collect stderr separately for each stage, or (b) collect stderr together for all stages (which could be interleaved, since stages run concurrently).  in either case, how to display it in less(1) is an open question.  there is also the question of whether we should try to do something special with (showing) stderr only when a stage has a non-zero exit code, or always, or whenever stderr is non-empty (how to detect this?!)...  this is an annoyingly harder problem than it feels like it should be, especially given how common it is to mess something up and get an error message on stderr.

  - maybe the way is to notice if any of the stages in the update pipeline exit with non-zero status (can't just wait for the whole update pipeline to exit, because if an earlier stage exits, later stages might continue processing.  or should we set pipefail to avoid that?!), and if that happens, kill less(1) (but not inotifywait!) and replace it with a less(1) invocation that shows the stderr (either all the separate ones, or all of them together is probably easier, at least to start with).


- multiline pipeline stages.  either: continuation lines start with leading whitespace, or, 1 or more blank lines betweens stages.


- add an easy way to manually invalidate the entire cache (easier than separately doing rm -rf .cache, that is)

- add an easy way to manually invalidate the cache for a particular stage.
  - done right, this can also solve the "volatile input stage" problem, where the initial command purposely changes on every invocation, and so caching its output is never appropriate.
  - eg. starting the stage's line with whitespace. which would require solving multiline stages with blank line(s) between stages.
  - another possibility is to prepend "nocache" to the stage.  at first i thought this could be achieved by having kenny provide (export) a `nocache` function, which just `rm`s the cache file, and then just runs (`exec`s) "$@".  but this won't work properly with complex shell syntax, or multiline stages.  so i think it will need special parsing.
  - another possibility is to have a leading _line_ `#nocache`.  this is probably a bit nicer.


- find a better name than "kenny" :(


- cmdline option to output a pretty-printed shell script that just runs the entire pipeline

- cmdline option to output an ugly shell oneliner that just runs the entire pipeline

- cmdline option to rewrite the given kenny file as a pretty-printed kenny file?


- if stdout isn't a tty, then don't send the output to `less`, just send it straight to stdout.  this is so that you can do `kenny foo | ...`, even though that's not ideal.  doing this already works fine if stdin is /dev/null (ie. if you do `kenny foo </dev/null | ...`), so maybe the easiest solution is to just do `exec</dev/null` if stdout isn't a tty.  except, probably not, once less(1) is reading from multiple fifos.  in that case, it's almost certainly better to just cat(1) the final fifo instead.

